# Audio AI Timeline

Here we will keep track of the latest AI models for waveform based audio generation, starting in 2023!

## 2023

| Date  | Release [Samples]                                                                                                                                                                              | Paper                                            | Code                                                                             | Trained Model                                                                                                                                                       |
| ----- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ | -------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 03.05 | [Diverse and Vivid Sound Generation from Text Descriptions](https://ligw1998.github.io/audiogeneration.html)                                                                                  | [arXiv](https://arxiv.org/abs/2305.01980)        | -                                                                                | -                                                                                                                                                                   |
| 25.04 | [AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head](https://github.com/AIGC-Audio/AudioGPT)                                                                        | [arXiv](https://arxiv.org/abs/2304.12995)        | [GitHub](https://github.com/AIGC-Audio/AudioGPT)                                  | [Hugging Face](https://huggingface.co/spaces/AIGC-Audio/AudioGPT)                                                                                                   |
| 24.04 | [TANGO: Text-to-Audio generation using instruction tuned LLM and Latent Diffusion Model](https://tango-web.github.io/)                                                                        | [PDF](https://openreview.net/pdf?id=1Sn2WqLku1e) | [GitHub](https://github.com/declare-lab/tango)                                   | [Hugging Face](https://huggingface.co/declare-lab/tango)                                                                                                            |
| 18.04 | [NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](https://speechresearch.github.io/naturalspeech2/)                                        | [arXiv](https://arxiv.org/abs/2304.09116)        | [GitHub (unofficial)](https://github.com/lucidrains/naturalspeech2-pytorch)        | -                                                                                                                                                                   |
| 10.04 | [Bark: Text-Prompted Generative Audio Model](https://github.com/suno-ai/bark)                                                                                                                  | -                                                | [GitHub](https://github.com/suno-ai/bark)                                        | [Hugging Face](https://huggingface.co/spaces/suno/bark) [Colab](https://colab.research.google.com/drive/1eJfA2XUa-mXwdMy7DoYKVYHI1iTd9Vkt?usp=sharing)              |
| 03.04 | [AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models](https://audit-demo.github.io/)                                                                                  | [arXiv](https://arxiv.org/abs/2304.00830)        | -                                                                                | -                                                                                                                                                                   |
| 29.03 | [Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos](https://sukun1045.github.io/video-physics-sound-diffusion/)                                                          | [arXiv](https://arxiv.org/abs/2303.16897)        | [GitHub](https://github.com/sukun1045/video-physics-sound-diffusion)              | -                                                                                                                                                                   |
| 08.03 | [VALL-E X: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling](https://vallex-demo.github.io/)                                                          | [arXiv](https://arxiv.org/abs/2303.03926)        | -                                                                                | -                                                                                                                                                                   |
| 27.02 | [I Hear Your True Colors: Image Guided Audio Generation](https://pages.cs.huji.ac.il/adiyoss-lab/im2wav/)                                                                                       | [arXiv](https://arxiv.org/abs/2211.03089)        | [GitHub](https://github.com/RoySheffer/im2wav)                                  | -                                                                                                                                                                   |
| 09.02 | ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models                                                                                                                           | [arXiv](https://arxiv.org/abs/2302.04456)        | -                                                                                | -                                                                                                                                                                   |
| 08.02 | [Noise2Music: Text-conditioned Music Generation with Diffusion Models](https://google-research.github.io/noise2music/)                                                                         | [arXiv](https://arxiv.org/abs/2302.03917)        | -                                                                                | -                                                                                                                                                                   |
| 04.02 | [Multi-Source Diffusion Models for Simultaneous Music Generation and Separation](https://gladia-research-group.github.io/multi-source-diffusion-models/)                                       | [arXiv](https://arxiv.org/abs/2302.02257)        | [GitHub](https://github.com/gladia-research-group/multi-source-diffusion-models) | -                                                                                                                                                                   |
| 30.01 | [SingSong: Generating musical accompaniments from singing](https://storage.googleapis.com/sing-song/index.html)                                                                                | [arXiv](https://arxiv.org/abs/2301.12662)        | -                                                                                | -                                                                                                                                                                   |
| 30.01 | [AudioLDM: Text-to-Audio Generation with Latent Diffusion Models](https://audioldm.github.io/)                                                                                                 | [arXiv](https://arxiv.org/abs/2301.12503)        | [GitHub](https://github.com/haoheliu/AudioLDM)                                   | [Hugging Face](https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation)                                                                            |
| 30.01 | [Mo√ªsai: Text-to-Music Generation with Long-Context Latent Diffusion](https://anonymous0.notion.site/Mo-sai-Text-to-Audio-with-Long-Context-Latent-Diffusion-b43dbc71caf94b5898f9e8de714ab5dc) | [arXiv](https://arxiv.org/abs/2301.11757)        | [GitHub](https://github.com/archinetai/audio-diffusion-pytorch)                  | -                                                                                                                                                                   |
| 29.01 | [Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models](https://text-to-audio.github.io/)                                                                              | [PDF](https://text-to-audio.github.io/paper.pdf) | -                                                                                | -                                                                                                                                                                   |
| 28.01 | [Noise2Music](https://noise2music.github.io/)                                                                                                                                                  | -                                                | -                                                                                | -                                                                                                                                                                   |
| 27.01 | [RAVE2](https://twitter.com/antoine_caillon/status/1618959533065535491?s=20&t=jMkPWBFuAH19HI9m5Sklmg) [[Samples RAVE1](https://anonymous84654.github.io/RAVE_anonymous/)]                      | [arXiv](https://arxiv.org/abs/2111.05011)        | [GitHub](https://github.com/acids-ircam/RAVE)                                    | -                                                                                                                                                                   |
| 26.01 | [MusicLM: Generating Music From Text](https://google-research.github.io/seanet/musiclm/examples/)                                                                                              | [arXiv](https://arxiv.org/abs/2301.11325)        | [GitHub (unofficial)](https://github.com/lucidrains/musiclm-pytorch)             | -                                                                                                                                                                   |
| 18.01 | [Msanii: High Fidelity Music Synthesis on a Shoestring Budget](https://kinyugo.github.io/msanii-demo/)                                                                                         | [arXiv](https://arxiv.org/abs/2301.06468)        | [GitHub](https://github.com/Kinyugo/msanii)                                      | [Hugging Face](https://huggingface.co/spaces/kinyugo/msanii) [Colab](https://colab.research.google.com/github/Kinyugo/msanii/blob/main/notebooks/msanii_demo.ipynb) |
| 16.01 | [ArchiSound: Audio Generation with Diffusion](https://flavioschneider.notion.site/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb)                                            | [arXiv](https://arxiv.org/abs/2301.13267)        | [GitHub](https://github.com/archinetai/audio-diffusion-pytorch)                  | -                                                                                                                                                                   |
| 05.01 | [VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](https://valle-demo.github.io/)                                                                                | [arXiv](https://arxiv.org/abs/2301.02111)        | [GitHub (unofficial)](https://github.com/lifeiteng/vall-e) [(demo)](https://lifeiteng.github.io/valle/index.html)                                                                               | -                                                                                                                                                                 |
| Date  | Release [Samples]                                                                                                                                                                            | Paper                                         | Code                                                                          | Trained Model                                                                                                                                                 |
| ----- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------- | ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 27.02 | Continuous descriptor-based control for deep audio synthesis                                                                                                                                 | [arXiv](https://arxiv.org/abs/2302.13542)        | -                                                                             | -                                                                                                                                                             |
| 09.02 | ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models                                                                                                                         | [arXiv](https://arxiv.org/abs/2302.04456)        | -                                                                             | -                                                                                                                                                             |
| 08.02 | [Noise2Music: Text-conditioned Music Generation with Diffusion Models](https://google-research.github.io/noise2music/)                                                                          | [arXiv](https://arxiv.org/abs/2302.03917)        | -                                                                             | -                                                                                                                                                             |
| 04.02 | [Multi-Source Diffusion Models for Simultaneous Music Generation and Separation](https://gladia-research-group.github.io/multi-source-diffusion-models/)                                        | [arXiv](https://arxiv.org/abs/2302.02257)        | [GitHub](https://github.com/gladia-research-group/multi-source-diffusion-models) | -                                                                                                                                                             |
| Date  | Release [Samples]                                                                                                                                                                            | Paper                                         | Code                                                                          | Trained Model                                                                                                                                                 |
| ----- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------- | ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 09.02 | ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models                                                                                                                         | [arXiv](https://arxiv.org/abs/2302.04456)        | -                                                                             | -                                                                                                                                                             |
| 08.02 | [Noise2Music: Text-conditioned Music Generation with Diffusion Models](https://google-research.github.io/noise2music/)                                                                          | [arXiv](https://arxiv.org/abs/2302.03917)        | -                                                                             | -                                                                                                                                                             |
| 04.02 | [Multi-Source Diffusion Models for Simultaneous Music Generation and Separation](https://gladia-research-group.github.io/multi-source-diffusion-models/)                                        | [arXiv](https://arxiv.org/abs/2302.02257)        | [GitHub](https://github.com/gladia-research-group/multi-source-diffusion-models) | -                                                                                                                                                             |
| 31.01 | [InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt](http://dongchaoyang.top/InstructTTS/)                                                       | [arXiv](https://arxiv.org/abs/2301.13662)        | [GitHub](https://github.com/yangdongchao/InstructTTS)                            | -                                                                                                                                                             |
| 30.01 | [SingSong: Generating musical accompaniments from singing](https://storage.googleapis.com/sing-song/index.html)                                                                                 | [arXiv](https://arxiv.org/abs/2301.12662)        | -                                                                             | -                                                                                                                                                             |
| 30.01 | [AudioLDM: Text-to-Audio Generation with Latent Diffusion Models](https://audioldm.github.io/)                                                                                                  | [arXiv](https://arxiv.org/abs/2301.12503)        | [GitHub](https://github.com/haoheliu/AudioLDM)                                   | [Hugging Face](https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation)                                                                         |
| 30.01 | [Mo√ªsai: Text-to-Music Generation with Long-Context Latent Diffusion](https://anonymous0.notion.site/Mo-sai-Text-to-Audio-with-Long-Context-Latent-Diffusion-b43dbc71caf94b5898f9e8de714ab5dc) | [arXiv](https://arxiv.org/abs/2301.11757)        | [GitHub](https://github.com/archinetai/audio-diffusion-pytorch)                  | -                                                                                                                                                             |
| 29.01 | [Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models](https://text-to-audio.github.io/)                                                                               | [PDF](https://text-to-audio.github.io/paper.pdf) | -                                                                             | -                                                                                                                                                             |
| 28.01 | [Noise2Music](https://noise2music.github.io/)                                                                                                                                                   | -                                             | -                                                                             | -                                                                                                                                                             |
| 27.01 | [RAVE2](https://twitter.com/antoine_caillon/status/1618959533065535491?s=20&t=jMkPWBFuAH19HI9m5Sklmg) [[Samples RAVE1](https://anonymous84654.github.io/RAVE_anonymous/)]                          | [arXiv](https://arxiv.org/abs/2111.05011)        | [GitHub](https://github.com/acids-ircam/RAVE)                                    | -                                                                                                                                                             |
| 26.01 | [MusicLM: Generating Music From Text](https://google-research.github.io/seanet/musiclm/examples/)                                                                                               | [arXiv](https://arxiv.org/abs/2301.11325)        | [GitHub (unofficial)](https://github.com/lucidrains/musiclm-pytorch)             | -                                                                                                                                                             |
| 18.01 | [Msanii: High Fidelity Music Synthesis on a Shoestring Budget](https://kinyugo.github.io/msanii-demo/)                                                                                          | [arXiv](https://arxiv.org/abs/2301.06468)        | [GitHub](https://github.com/Kinyugo/msanii)                                      | [Hugging Face](https://huggingface.co/spaces/kinyugo/msanii) [Colab](https://colab.research.google.com/github/Kinyugo/msanii/blob/main/notebooks/msanii_demo.ipynb) |
| 16.01 | [ArchiSound: Audio Generation with Diffusion](https://flavioschneider.notion.site/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb)                                             | [arXiv](https://arxiv.org/abs/2301.13267)        | [GitHub](https://github.com/archinetai/audio-diffusion-pytorch)                  | -                                                                                                                                                             |
| 05.01 | [VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](https://valle-demo.github.io/)                                                                                 | [arXiv](https://arxiv.org/abs/2301.02111)        | -                                                                             | -                                                                                                                                                             |
